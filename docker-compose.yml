version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: private_llm_engine
    restart: unless-stopped

    # Security: no host ports exposed -> only internal network access.
    # Nginx is the only ingress point.
    volumes:
      - ollama_data:/root/.ollama

    networks:
      - ai_secure_net

    # Optional GPU support (works when NVIDIA runtime is available).
    # In CI we override this with docker-compose.ci.yml.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  nginx:
    image: nginx:alpine
    container_name: ai_proxy
    restart: unless-stopped
    depends_on:
      - ollama

    ports:
      - "80:80"
      - "443:443"

    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro

    networks:
      - ai_secure_net

networks:
  ai_secure_net:
    driver: bridge

volumes:
  ollama_data:
